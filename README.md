# ğŸ“š Paper Summarizer

An intelligent research paper analyzer using LlamaParse and LlamaIndex with hierarchical chunking and auto-merging retrieval, powered by Google Gemini AI.

## Features

- **LlamaParse**: Layout-aware PDF parsing that preserves tables, headers, and structure
- **Hierarchical Chunking**: Parent-child chunk structure for better context
- **Auto-Merging Retriever**: Automatically "zooms out" to larger context when needed
- **Gemini AI**: Uses gemini-2.0-flash for chat and text-embedding-004 for embeddings
- **React Frontend**: Modern UI with Tailwind CSS
- **Persistent Storage**: Process once, chat forever

## ğŸš€ Deployment

### Option 1: Deploy Backend on Render

1. Create a new **Web Service** on [Render](https://render.com)
2. Connect your GitHub repo: `sohaibsultan43/Papaer_Summarizer`
3. Configure:
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `uvicorn api:app --host 0.0.0.0 --port $PORT`
4. Set environment variables:
   - `LLAMA_CLOUD_API_KEY`: Your LlamaCloud API key
   - `GOOGLE_API_KEY`: Your Google AI API key
5. Deploy!

### Option 2: Deploy Frontend on Vercel

1. Go to [Vercel](https://vercel.com) and import your repo
2. Set **Root Directory** to `frontend`
3. Set environment variable:
   - `VITE_API_URL`: Your Render backend URL (e.g., `https://paper-summarizer.onrender.com`)
4. Deploy!

## ğŸ’» Local Development

### 1. Clone and Setup

```bash
git clone https://github.com/sohaibsultan43/Papaer_Summarizer.git
cd Papaer_Summarizer

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys

Create a `.env` file:

```env
LLAMA_CLOUD_API_KEY=llx-your-key-here
GOOGLE_API_KEY=your-google-ai-key-here
```

**Get your keys:**
- **LlamaCloud**: [cloud.llamaindex.ai](https://cloud.llamaindex.ai) â†’ API Key â†’ Generate New Key
- **Google AI**: [aistudio.google.com](https://aistudio.google.com/app/apikey)

### 3. Run the Backend

```bash
python api.py
```

Backend will be available at `http://localhost:8000`

### 4. Run the Frontend

```bash
cd frontend
npm install
npm run dev
```

Frontend will be available at `http://localhost:5173`

## ğŸ“ File Structure

```
Paper_Summarizer/
â”œâ”€â”€ api.py              # FastAPI backend
â”œâ”€â”€ ingest.py           # PDF processing (used by API)
â”œâ”€â”€ chat.py             # CLI chat interface
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ Procfile            # For Render deployment
â”œâ”€â”€ .env                # API keys (not committed)
â”œâ”€â”€ storage/            # Saved indexes (per paper)
â”œâ”€â”€ uploads/            # Uploaded PDFs
â”œâ”€â”€ Papers/             # Sample papers
â””â”€â”€ frontend/           # React + Vite frontend
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.jsx     # Main React component
    â”‚   â””â”€â”€ api.js      # API client
    â”œâ”€â”€ vercel.json     # Vercel config
    â””â”€â”€ package.json
```

## ğŸ”§ How It Works

### Hierarchical Chunking

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Parent (1024)     â”‚  â† Full context
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼               â–¼               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Mid(512) â”‚    â”‚ Mid(512) â”‚    â”‚ Mid(512) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚               â”‚               â”‚
        â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
        â–¼           â–¼   â–¼           â–¼   â–¼           â–¼
     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â” ...                      â”Œâ”€â”€â”€â”€â”€â”
     â”‚Leaf â”‚     â”‚Leaf â”‚  â† Search happens here  â”‚Leaf â”‚
     â”‚(256)â”‚     â”‚(256)â”‚                         â”‚(256)â”‚
     â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”˜
```

### Auto-Merging Retrieval

When multiple leaf chunks from the same parent match your query, the retriever automatically "merges" up to return the parent chunk instead. This gives you better context without losing precision.

## ğŸ› ï¸ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/papers` | GET | List all processed papers |
| `/upload` | POST | Upload and process a new PDF |
| `/chat` | POST | Ask a question about a paper |
| `/papers/{id}` | DELETE | Delete a paper and its index |

## ğŸ“ CLI Usage

### Process a PDF manually

```bash
python ingest.py path/to/paper.pdf
```

### Chat via command line

```bash
python chat.py paper_name
```

## ğŸ› Troubleshooting

**"Index not found"**: Upload a PDF first via the web UI or run `ingest.py`.

**"API key not found"**: Make sure your `.env` file has valid keys.

**Slow parsing**: LlamaParse processes asynchronously. Large PDFs may take 1-2 minutes.

## ğŸ“Š Free Tier Limits

- **LlamaParse**: 1,000 pages/day
- **Google AI (Gemini)**: Generous free tier for development
